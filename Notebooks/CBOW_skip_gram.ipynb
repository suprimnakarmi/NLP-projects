{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suprimnakarmi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Word embedding made significant improvement in many NLP tasks possible. Its understanding of word schematics and ability to represent different length texts to fixed vectors made it very popular among many complex NLP tasks. Most of the machine learning algorithms can be directly applied to word embeddings for classification and regressions tasks as the length of vector is fixed. In this blog, we will try to look at the packages which help us implement Word2Vec using 2 popular methods named CBOW and Skip-Gram. Also we will look at some properties and visualisations of embeddings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r\"[^.A-Za-z]\", ' ', text)\n",
    "sentence=text.split('.')\n",
    "tokens = [nltk.word_tokenize(words) for words in sentence]\n",
    "# print(tokens)\n",
    "model = Word2Vec(tokens, vector_size=50, sg=1, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.build_vocab(sentence, progress_per=10000) # Create vocabulary table (filter out unique word, perform basic counts on them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sentence, total_examples=model.corpus_count, epochs=40, report_delay=1) # Train word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/wvyv5cg92s56wgpczsbkvjdr0000gp/T/ipykernel_65762/1096592852.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True) # Make model moer memory efficient\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True) # Make model moer memory efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualisations: 0.2397730052471161\n",
      "can: 0.22353295981884003\n",
      "Vec: 0.21945615112781525\n",
      "this: 0.1952129602432251\n",
      "NLP: 0.18510271608829498\n",
      "complex: 0.16958557069301605\n",
      "will: 0.168027862906456\n",
      "us: 0.16670414805412292\n",
      "represent: 0.1430751234292984\n",
      "the: 0.13658297061920166\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar(positive=[\"many\"])\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'tasks',\n",
       " 'the',\n",
       " 'Word',\n",
       " 'we',\n",
       " 'many',\n",
       " 'NLP',\n",
       " 'fixed',\n",
       " 'popular',\n",
       " 'made',\n",
       " 'word',\n",
       " 'length',\n",
       " 'at',\n",
       " 'look',\n",
       " 'embeddings',\n",
       " 'will',\n",
       " 'vectors',\n",
       " 'it',\n",
       " 'very',\n",
       " 'among',\n",
       " 'complex',\n",
       " 'Most',\n",
       " 'texts',\n",
       " 'improvement',\n",
       " 'different',\n",
       " 'represent',\n",
       " 'embedding',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'schematics',\n",
       " 'understanding',\n",
       " 'Its',\n",
       " 'possible',\n",
       " 'significant',\n",
       " 'ability',\n",
       " 'visualisations',\n",
       " 'learning',\n",
       " 'packages',\n",
       " 'some',\n",
       " 'Also',\n",
       " 'Gram',\n",
       " 'Skip',\n",
       " 'CBOW',\n",
       " 'named',\n",
       " 'methods',\n",
       " 'using',\n",
       " 'Vec',\n",
       " 'implement',\n",
       " 'us',\n",
       " 'help',\n",
       " 'which',\n",
       " 'try',\n",
       " 'algorithms',\n",
       " 'blog',\n",
       " 'this',\n",
       " 'In',\n",
       " 'is',\n",
       " 'vector',\n",
       " 'as',\n",
       " 'regressions',\n",
       " 'classification',\n",
       " 'for',\n",
       " 'applied',\n",
       " 'directly',\n",
       " 'be',\n",
       " 'properties',\n",
       " 'can']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
